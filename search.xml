<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>剑影舞动，揭秘karmada</title>
      <link href="/2024/01/16/%E5%89%91%E5%BD%B1%E8%88%9E%E5%8A%A8%EF%BC%8C%E6%8F%AD%E7%A7%98karmada/"/>
      <url>/2024/01/16/%E5%89%91%E5%BD%B1%E8%88%9E%E5%8A%A8%EF%BC%8C%E6%8F%AD%E7%A7%98karmada/</url>
      
        <content type="html"><![CDATA[<iframe width="1280" height="720" src="https://www.youtube.com/embed/YAxNwRb93xk" title="Butter-Fly / 和田光司【Animelo Summer Live 2014 -ONENESS- Day1】" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><h1 id="karmada-多云k8s"><a href="#karmada-多云k8s" class="headerlink" title="karmada-多云k8s"></a>karmada-多云k8s</h1><p>Karmada（Kubernetes Armada舰队）</p><p>Karmada吸取了CNCF社区的Federation v1和v2（也称为kubefed）项目经验与教训,正式发布则是2021年4月25日在深圳召开的华为开发者大会（HDC.Cloud）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">阿里云ack的多集群管理，实现git，helm的版本化管理</span><br><span class="line">多集群多数是指集群纳管，可以创建或者导入很多集群，做一些统一的认证，查看，或者是指定资源部署位置</span><br><span class="line">而集群联邦是应用视角的管理，比如权重分配，流量分发，差异化配置等</span><br><span class="line"></span><br><span class="line">华为云的mcp</span><br></pre></td></tr></table></figure><!-- ![image-20210717134154167](..\typora-user-images\image-20210717134154167.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/karmada.png"></p><blockquote><p>聚合api：解决api碎片化的问题，</p><p>应用负载：应用跨机房的负载，金融领域要求三异地副本的要求，确保同一业务的不同实例会均衡分发到两地三中心的不同资源域，确保单个存储、单个集群甚至单个数据中心发生故障时，不会影响业务的整体可用性。</p><p>多集群调度：跨az，跨region的高可用</p><p>策略管理：权限管理</p><p>云边:边缘设备</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">为什么引入karmada：</span><br><span class="line">集群繁多的重复劳动，多集群联邦</span><br><span class="line">karmada只能控制dp的数量吗</span><br><span class="line">多集群的cm自动同步</span><br><span class="line">K8s规定5000个node，现实中超过5000的使用karmada</span><br><span class="line">包括cluster、policy、binding、works等多种CRD资源作为管理端资源对象，还有哪些crd</span><br><span class="line">使用说明：</span><br><span class="line">PropagationPolicy通过spec中的resourceSelectors声明作用的对象，其中clusterAffinity:集群亲和性，</span><br><span class="line"></span><br><span class="line">示例文件见：github</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">https://blog.csdn.net/devcloud/article/details/118603090</span><br><span class="line">4 月 25 日，在华为开发者大会 2021（Cloud）上，华为云 CTO 张宇昕宣布云原生多云容器编排项目 Karmada 正式开源。Karmada项目融入华为及工商银行、小红书、中国一汽等各企业在多云管理领域的丰富积累，为开发者提供详实有效的实践指导与帮助，未来还计划将该项目捐赠给云原生计算基金会 CNCF。</span><br><span class="line"></span><br><span class="line">k8s官方宣称支持最大150000个pods，5000个node。但是现实生产环境中业务时常有超过该规模的述求，比如说大型电商如淘宝，拼多多，又比如AI和大数据处理的workflow。同时出于合作和业务述求，一家公司还有可能将业务部署到不同的云厂商，或者自建机房和公有云配合使用。因此k8s多集群方案也是云原生领域的一个热点</span><br><span class="line"></span><br><span class="line">根据最新的调查报告显示，超过93%的企业正同时使用多个云厂商的服务,</span><br><span class="line"></span><br><span class="line">一群孤岛，威尼斯水城(统一应用交付，部署运维)，大航海时代</span><br><span class="line">集群边界限制，资源调度受限于集群，</span><br><span class="line">厂商绑定，缺少自动的故障迁移，</span><br></pre></td></tr></table></figure><!-- ![image-20210716105056507](..\typora-user-images\image-20210716105056507.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/rancher%20vs.png"></p><blockquote><p>rancher 对于我们已经存量有这么多的k8s集群来说，改造成本比较大，而且太重了</p></blockquote><h2 id="概念-以类似k8s的方式进行部署"><a href="#概念-以类似k8s的方式进行部署" class="headerlink" title="概念-以类似k8s的方式进行部署"></a>概念-以类似k8s的方式进行部署</h2><p><a href="https://gitee.com/mirrors/Karmada">https://gitee.com/mirrors/Karmada</a></p><!-- ![image-20210716105726510](..\typora-user-images\image-20210716105726510.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/rancher%20vs.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">The Karmada Control Plane consists of the following components:</span><br><span class="line">Karmada API Server</span><br><span class="line">Karmada Controller Manager</span><br><span class="line">Karmada Scheduler</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">在我个人的理解上，Karmada有以下优势：</span><br><span class="line">1）Karmada以类k8s的形式部署，它有API Server、Controller Manager等，对我们已经拥有存量那么多k8s集群的企业来说，改造成本是比较小的，我们只需在上面部署一个管理集群即可</span><br><span class="line">2）Karmada-Controller-Manager管理包括cluster、policy、binding、works等多种CRD资源作为管理端资源对象，但是它没有侵入到原生的我们想要部署的那些k8s原生资源对象。</span><br><span class="line">3）Karmada仅管理资源在集群间的调度，子集群内分配高度自治</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Karmada整体的Resources是如何进行分发的？</span><br><span class="line">第一步，集群注册到Karmada</span><br><span class="line">第二步，定义Resource Template</span><br><span class="line">第三步，制定分发策略Propagation Policy</span><br><span class="line">第四步，制定Override策略</span><br><span class="line">第五步，看Karmada干活</span><br><span class="line"></span><br><span class="line">还支持集群容忍，按集群标签、故障域分发</span><br><span class="line"></span><br><span class="line">新增cluter的资源类型，</span><br><span class="line">多集群环境下的集群容器网络</span><br><span class="line"></span><br><span class="line">多集群的监控，多集群的日志收集是2021年q4的</span><br><span class="line"></span><br><span class="line">karmada是否会成为单点，多云管理是不是就挂了，不会的，底层的pod不会挂</span><br><span class="line">阿里云性能好，华为云差一点，多云的调度是不是会有权重，还是说依旧安装prapagation调度</span><br><span class="line">数据库关键资源做跨集群访问成为瓶颈</span><br><span class="line"></span><br><span class="line">云区域（region)，可用区（AZ），跨区域数据复制（Cross-region replication）与灾备（Disaster Recovery）（部分1）</span><br><span class="line">https://www.cnblogs.com/sammyliu/p/8902556.html</span><br><span class="line">https://mp.weixin.qq.com/s/SdCacTFZCH5NGdMWky_JAw</span><br><span class="line">ebay istio网络云原生架构</span><br><span class="line">https://mp.weixin.qq.com/s/lruZzIr5T6bHwMAe8lDvZQ</span><br><span class="line"></span><br><span class="line">F5 GTM的DNS智能解析在双活数据中心中的应用</span><br><span class="line">fat container</span><br><span class="line">Service Mesh实际上是一种SDN，等同于OSI模型中的会话层</span><br><span class="line">在Pod内部sidecar会与应用容器之间建立本地TCP连接，其中使用mTLS（双向传输层加密）</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/opskam.png"></p><!-- ![image-20210716115129867](..\typora-user-images\image-20210716115129867.png) --><!-- ![image-20210716115221161](..\typora-user-images\image-20210716115221161.png) --><h2 id="有哪些crd"><a href="#有哪些crd" class="headerlink" title="有哪些crd"></a>有哪些crd</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">OverridePolicy</span><br><span class="line">没有OverridePolicy，则worker集群中的deployment和global的deployment的spec相同，但是我们有可能是要针对worker集群的不同或者业务需求修改内容。</span><br><span class="line">比如现在我们修改cluster1中的deployment的image为nginx:test，用来做一个灰度发布。则我们可以创建一个如下的</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@192 nginx]# kubectl get  crd</span><br><span class="line">NAME                                           CREATED AT</span><br><span class="line">clusteroverridepolicies.policy.karmada.io      2021-10-12T06:57:34Z</span><br><span class="line">clusterpropagationpolicies.policy.karmada.io   2021-10-12T06:57:34Z</span><br><span class="line">clusterresourcebindings.work.karmada.io        2021-10-12T06:57:34Z</span><br><span class="line">clusters.cluster.karmada.io                    2021-10-12T06:57:34Z</span><br><span class="line">overridepolicies.policy.karmada.io             2021-10-12T06:57:34Z</span><br><span class="line">propagationpolicies.policy.karmada.io          2021-10-12T06:57:34Z</span><br><span class="line">replicaschedulingpolicies.policy.karmada.io    2021-10-12T06:57:34Z</span><br><span class="line">resourcebindings.work.karmada.io               2021-10-12T06:57:34Z</span><br><span class="line">serviceexports.multicluster.x-k8s.io           2021-10-12T06:57:34Z</span><br><span class="line">serviceimports.multicluster.x-k8s.io           2021-10-12T06:57:34Z</span><br><span class="line">works.work.karmada.io                          2021-10-12T06:57:34Z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">针对各种crd设置PropagationPolicy分发，使用ClusterPropagationPolicy来分发，ClusterPropagationPolicy对象是cluster scope的</span><br><span class="line">两者的区别是：</span><br><span class="line">它们都用于保存传播声明，但它们具有不同的功能：</span><br><span class="line">PropagationPolicy：只能代表同一命名空间内资源的传播策略。比如传播dp</span><br><span class="line">ClusterPropagationPolicy：可以代表所有资源的传播策略，包括命名空间范围和集群范围的资源。比如传输crd</span><br><span class="line"></span><br><span class="line">override资源的说明：</span><br><span class="line">https://github.com/karmada-io/karmada/blob/master/docs/override-policy.md</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">跨集群服务发现</span><br><span class="line">我在member1上有deploy和svc，想通过member2访问member1</span><br><span class="line">下面是操作步骤</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="工商银行多云环境"><a href="#工商银行多云环境" class="headerlink" title="工商银行多云环境"></a>工商银行多云环境</h2><blockquote><p><a href="https://www.freeaihub.com/post/110554.html">https://www.freeaihub.com/post/110554.html</a></p></blockquote><p>karmada的工作原理也查看上面的链接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">工行金融云成效</span><br><span class="line">在容器云方面，工行的金融云成效是非常大的，首先体现在入云规模大，截至目前应用平台云容器规模超20万，业务容器占到55,000个左右，整体的一些核心业务都已入容器云内部。除了在同业的入云规模最大之外，我们的业务场景也涉及非常广泛，一些核心的应用，核心的银行业务系统，包括个人金融体系的账户，快捷支付、线上渠道、纪念币预约等，也已容器化部署；另外，我们的一些核心技术支撑类应用如MySQL，还有一些中间件以及微服务框架也已入云；此外，还有一些新技术领域，包括物联网、人工智能、大数据等。</span><br><span class="line"></span><br><span class="line">但现有的解决方案对上层应用还是暴露出非常多的问题：</span><br><span class="line">1）对上层应用来说，它可能上了容器云比较关心的一个部分，就是我们具有在业务峰值的过程中自动伸缩的能力，但自动伸缩现在只是在集群内部，没有整体的跨集群自动伸缩的能力。</span><br><span class="line">2）无跨集群自动调度能力，包括调度的能力可能都是在集群内部，应用需要自主的选择具体的集群</span><br><span class="line">3）集群对上层用户不透明</span><br><span class="line">4）无跨集群故障自动迁移能力。我们还是主要依靠两地三中心架构上副本的冗余，那么在故障恢复的自动化恢复过程中，这一块的高可用能力是有缺失的。</span><br></pre></td></tr></table></figure><!-- ![image-20210716113004617](..\typora-user-images\image-20210716113004617.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/opskamrada.png"></p><!-- ![image-20210716113501581](..\typora-user-images\image-20210716113501581.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/ops202401160941420.png"></p><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><p>参考链接：</p><p><a href="https://mp.weixin.qq.com/s/lpg-jKEXHDa-F8riNtQWsw">https://mp.weixin.qq.com/s/lpg-jKEXHDa-F8riNtQWsw</a></p><h3 id="启动本地k8s集群local-up-karmada-sh"><a href="#启动本地k8s集群local-up-karmada-sh" class="headerlink" title="启动本地k8s集群local-up-karmada.sh"></a>启动本地k8s集群local-up-karmada.sh</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">依赖安装包:go和kind</span><br><span class="line">Prerequisites</span><br><span class="line">Go version v1.14+</span><br><span class="line">kubectl version v1.19+</span><br><span class="line">kind version v0.11.1（必须使用这个版本，可以从浏览器下载包再导入到服务器上）</span><br><span class="line"></span><br><span class="line">如果机器重启的话，localup起来的k8s集群会失效，restart docker也不启用，这个时候可以继续再执行local-up-karmada.sh脚本即可</span><br><span class="line">、</span><br><span class="line">从码云上拉取代码</span><br><span class="line">git clone https://gitee.com/mirrors/Karmada.git</span><br><span class="line">从github上拉取代码，比较慢</span><br><span class="line">git clone https://github.com/karmada-io/karmada.git karmada-io/karmada</span><br><span class="line">cd karmada-io/karmada</span><br><span class="line">与社区给的示例不同,karmada 快速安装脚本，把 Karmada 安装到现有的 Kubernetes 集群，Karmada 本身与 Kubernetes 类似，可以理解成 Kind。</span><br><span class="line">$ cd karmada</span><br><span class="line"></span><br><span class="line">设置代理</span><br><span class="line">go env -w GO111MODULE=on</span><br><span class="line">go env -w GOPROXY=https://goproxy.io,direct</span><br><span class="line">$ ./hack/local-up-karmada.sh</span><br><span class="line"></span><br><span class="line">这里包含一系列步骤： </span><br><span class="line">1）检查go、kind等工具是否已经存在</span><br><span class="line">2）调用kind创建host k8s集群，集群版本默认为1.19.1，与karmada重用的k8s组件（kube-apiserver、kube-controllermanager）版本一致 。创建k8s集群需要的kindest/node:v1.19.1镜像较大，可以提前下载好，防止后续的local-up-karmada等待集群启动超时（默认5分钟）</span><br><span class="line">3）build karmada控制面可执行文件及容器镜像，build结束后本地可以找到如下镜像：karmada-agent、karmada-webhook、karmada-scheduler、karmada-controller-manager</span><br><span class="line">4）部署karmada控制面组件到host集群</span><br><span class="line">5）创建CRD，也就是karmada自定义的多云工作负载API资源，包含：propgation policy，override policy，work，resource binding等</span><br><span class="line">6）创建webhook</span><br><span class="line">7）部署完成后，形成kubeconfig文件$HOME/kube/karmada.config，包含karmada-host和karmada-apiserver两个context，分别对应支撑karmada控制面运行的host集群，以及karmada控制面本身</span><br><span class="line">注意：karmada还提供了remote-up-karmada.sh脚本，用以把一个现有的k8s集群加入联邦。感兴趣的读者可以阅读karmada项目的readme尝试</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@localhost karmada]# kubectl get nodes</span><br><span class="line">NAME                         STATUS   ROLES    AGE    VERSION</span><br><span class="line">karmada-host-control-plane   Ready    master   102m   v1.18.2</span><br><span class="line"></span><br><span class="line">[root@localhost karmada]# kubectl config  get-clusters</span><br><span class="line">NAME</span><br><span class="line">kind-karmada-host</span><br><span class="line"></span><br><span class="line">部署karmada完成后，在切换到karmada-host context后，执行kubectl get po --all-namespaces可以得到已经部署的组件列表：</span><br></pre></td></tr></table></figure><!-- ![image-20210716123146981](..\typora-user-images\image-20210716123146981.png)![image-20211009145523958](image-20211009145523958.png)Member1-3使用的是kindnet网络插件![image-20211009185755296](08%E3%80%81k8s%E9%A1%B9.assets/image-20211009185755296.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/ops%E9%83%A8%E7%BD%B2karma%E5%A4%A7.png"></p><h4 id="安装注意事项"><a href="#安装注意事项" class="headerlink" title="安装注意事项"></a>安装注意事项</h4><p>遇到这个问题处理办法</p><!-- ![image-20211009143155587](image-20211009143155587.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/ops202401160943207.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">直接执行代理，然后继续执行安装</span><br><span class="line">go env -w GO111MODULE=on</span><br><span class="line">go env -w GOPROXY=https://goproxy.io,direct</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">执行local-up脚本是需要消耗大约20分钟时间的，中间会build karmada控制面可执行文件及容器镜像，build结束后本地可以找到karmada的镜像</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="执行卡主"><a href="#执行卡主" class="headerlink" title="执行卡主"></a>执行卡主</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果长时间卡在 Ensuring node image (kindest/node:v1.18.2) 这个步骤，可以使用 docker pull kindest/node:v1.18.2 来得到镜像拉取进度条。</span><br><span class="line"></span><br></pre></td></tr></table></figure><!-- ![image-20210716124055889](..\typora-user-images\image-20210716124055889.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/karmda.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">多次启动local-up集群的时候，应用卡等host-cluster集群准备完毕这一步骤了</span><br><span class="line"></span><br><span class="line">make: 离开目录“/app/karmada/Karmada”</span><br><span class="line">Waiting for the host clusters to be ready...</span><br><span class="line">Waiting for kubeconfig file /root/.kube/karmada.config and clusters karmada-host to be ready...</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="启动远程k8s集群remote-up-karmada-sh"><a href="#启动远程k8s集群remote-up-karmada-sh" class="headerlink" title="启动远程k8s集群remote-up-karmada.sh"></a>启动远程k8s集群remote-up-karmada.sh</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">再本地的k8s环境中启karmada程序</span><br><span class="line"></span><br><span class="line">./hack/remote-up-karmada.sh ~/.kube/config local</span><br><span class="line">安装脚本将下载golang包，如果您的服务器在中国大陆，您可以设置去代理，如此 export GOPROXY=https://goproxy.cn</span><br><span class="line"></span><br><span class="line">部署的时候etcd启动有问题,因为我机器的etcd端口已经被占用了，就导致没有etcd端口使用</span><br></pre></td></tr></table></figure><h3 id="kindnet"><a href="#kindnet" class="headerlink" title="kindnet"></a>kindnet</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">② kindnet：为Kubernetes提供一个简单的CNI插件，该插件具有IPv4和IPv6支持，可提供集群网络。它的诞生，是因为当前的CNI插件缺少IPv6支持，并且没有自动替代方案来创建具有IPv6的多节点kubernetes集群。当所有群集节点都属于同一子网时，该插件仅在“简单”网络环境中工作。作为嵌入式ipmasq代理，它是KIND的默认CNI插件。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="karmada使用"><a href="#karmada使用" class="headerlink" title="karmada使用"></a>karmada使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用karmada管理的多云环境包含两类集群：</span><br><span class="line">host集群：即由karmada控制面构成的集群，接受用户提交的工作负载部署需求，将之同步到member集群，并从member集群同步工作负载后续的运行状况。</span><br><span class="line">member集群：由一个或多个k8s集群构成，负责运行用户提交的工作负载</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">安装成功界面</span><br><span class="line">Local Karmada is running.</span><br><span class="line">To start using your karmada, run:</span><br><span class="line">   export KUBECONFIG=/root/.kube/karmada.config</span><br><span class="line">Please use &#x27;kubectl config use-context karmada-host/karmada-apiserver&#x27; to switch the host and control plane cluster.</span><br><span class="line">切到karmada控制面：</span><br><span class="line">kubectl config use-context karmada-apiserver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To manage your member clusters, run:</span><br><span class="line">  export KUBECONFIG=/root/.kube/members.config</span><br><span class="line">Please use &#x27;kubectl config use-context member1/member2/member3&#x27; to switch to the different member cluster.</span><br><span class="line"></span><br><span class="line">查看创建后的k8s集群有哪些，（karmada安装之后默认是有3个cluster，都加入到karmada的管理中）</span><br><span class="line">[root@192 Karmada]# export KUBECONFIG=/root/.kube/karmada.config</span><br><span class="line">[root@192 Karmada]# kubectl get clusters</span><br><span class="line">NAME      VERSION   MODE   READY   AGE</span><br><span class="line">member1   v1.19.1   Push   True    51m</span><br><span class="line">member2   v1.19.1   Push   True    51m</span><br><span class="line">member3   v1.19.1   Pull   True    49m</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@192 .kube]# kubectl get node</span><br><span class="line">NAME                         STATUS   ROLES    AGE   VERSION</span><br><span class="line">karmada-host-control-plane   Ready    master   14m   v1.19.1</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/USE_KARMADA.png"></p><!-- ![image-20211009150116029](image-20211009150116029.png) --><p>karmada有两个控制面，</p><blockquote><p>1、管理karmada控制面</p><p>karmada-host context 对应的控制面运行在kube-system namespace 中，用来运行管理 karmada 控制面，是个由 kind 启动的标准的 k8s 管理节点。</p><p>2、karmada控制面</p><p>karmada-apiserver context 对应的控制面运行在karmada-system namespace 中，是 karmada 控制面，也就是 karmada readme 中提到的 host 集群。它由local-up-karmada.sh脚本部署到karmada-host集群中。该控制面重用了 k8s 的两个组件：kube-apiserver和kube-controllermanager以及 etcd，其他 3 个为 karmada 组件，包括kamada-controller-manager、karmada-scheduler、karmada-webhook﻿，</p><p>所有后续集群联邦相关的操作，包括用karmadactl发出的member集群管理请求，以及用kubectl发出的工作负载管理请求都发往karmada控制面，这些请求存放在上面的etcd-0这个pod中，</p></blockquote><!-- ![image-20211009150729700](image-20211009150729700.png)  --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/USE_KARMADA.png"></p><blockquote><p>启了三个member集群</p></blockquote><h3 id="纳管"><a href="#纳管" class="headerlink" title="纳管"></a>纳管</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">用户可以用 karmadactl 和 kubectl 两个 cli 使用 karmada，其中： </span><br><span class="line">1.karmadactl 用来执行 member 集群的加入(joint)/离开（unjoin）、标志一个 member 集群不可调度（cordon）或解除不可调度的标志（uncordon）</span><br><span class="line">2.kubectl 用来向 karmada 集群提交标准的 k8s 资源请求，或由 karmada 定义的 CR 请求。用以管理 karmada 集群中的工作负载。</span><br><span class="line"></span><br><span class="line">3.1 使用karmadactl管理member集群</span><br><span class="line">export KUBECONFIG=/root/.kube/members.config</span><br><span class="line">1.执行hack/create-cluster.sh member1 $HOME/.kube/karmada.config创建新的集群member1</span><br><span class="line">2.执行kubectl config use-context karmada-apiserver切换到karmada控制面</span><br><span class="line">3.执行karmadactl join member1 --cluster-kubeconfig=$HOME/.kube/karmada.config以push的方式把member1加入karmada集群 </span><br><span class="line">注意：如果还没有编译过karmadactl可执行文件，可以在前面git下载代码根目录执行make karmadactl</span><br><span class="line">注意：karmada中的host集群和member集群之间的同步方式有push和pull两种，这里的上手过程采用push的方式，感兴趣的读者可以参考karmada的readme尝试pull同步方式</span><br><span class="line"></span><br><span class="line">目前karmadactl没有list member集群的功能，对于已经加入的member集群，可以通过获取Cluster类型的CR实现：</span><br><span class="line">kubectl get clusters</span><br><span class="line">得到输出为：</span><br><span class="line">NAME      VERSION  MODE  READY  AGE</span><br><span class="line">member1  v1.19.1  Push  True    88s</span><br><span class="line">上面的create-cluster.sh脚本默认创建最新版的k8s集群，为了避免再次拉下一个大镜像，可以修改create-cluster.sh脚本，为kind create cluster命令加上--image=&quot;kindest/node:v1.19.1&quot;参数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">使用kubectl管理工作负载</span><br><span class="line">karmada代码里自带一个nginx应用可以用来体验基于karmada的分布式工作负载管理：</span><br><span class="line">1.执行kubectl config use-context karmada-apiserver切换到karmada控制面</span><br><span class="line"></span><br><span class="line">2.执行kubectl create -f samples/nginx/deployment.yaml创建deployment资源如前面所述，由于kamada控制面没有部署deployment controller，nginx不会在karmada控制面所在集群跑起来，而是仅仅保存在etcd里这时候如果去member1集群查看pod资源的情况，可以发现nginx也没有在member1集群中运行起来</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">切换到karmada的控制面：</span><br><span class="line">kubectx karmada-apiserver</span><br><span class="line">kubectl create -f samples/nginx/propagationpolicy.yaml</span><br><span class="line">要有propaga资源才可以做传播策略</span><br><span class="line">[root@192 Karmada]# kubectl api-resources|grep propaga</span><br><span class="line">clusterpropagationpolicies        cpp          policy.karmada.io              false        ClusterPropagationPolicy</span><br><span class="line">propagationpolicies               pp           policy.karmada.io              true         PropagationPolicy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">karmada-host is only used for debugging（用于调试和安装）</span><br><span class="line">karmada-host is only used when you need to install/modify karmada components.</span><br><span class="line">karmada-apiserver is the one you will use most often.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">传播策略中制定起了2个dp，规定member1和member2中各一个dp</span><br><span class="line"></span><br><span class="line">kubectl get propagationpolicies -o yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>创建一个member-zyx的k8s集群(大约需要10分钟,默认使用的是weave插件)</p><!-- ![image-20211009154337544](image-20211009154337544.png)   --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/karmada202401161006084.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看新建集群的情况</span><br><span class="line">kubectl cluster-info --context member-zyx --kubeconfig /root/.kube/karmada.config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果这里创建集群失败之后，发现get pod的时候local-host的集群消失了，这个之后重新执行./hack/local-up-karmada.sh命令</span><br></pre></td></tr></table></figure><h3 id="纳管集群"><a href="#纳管集群" class="headerlink" title="纳管集群"></a>纳管集群</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">使用push的方式将一个集群纳管到karmada中(再控制面操作，且制定配置文件的时候使用$home变量)</span><br><span class="line">[root@192 Karmada]# kubectx</span><br><span class="line">karmada-apiserver</span><br><span class="line">karmada-host</span><br><span class="line">[root@192 Karmada]# kubectl get cluster</span><br><span class="line">NAME      VERSION   MODE   READY   AGE</span><br><span class="line">member1   v1.19.1   Push   True    27h</span><br><span class="line">member2   v1.19.1   Push   True    27h</span><br><span class="line">member3   v1.19.1   Pull   True    27h</span><br><span class="line">[root@192 Karmada]# ./karmadactl join local --cluster-kubeconfig=$HOME/.kube/config </span><br><span class="line">[root@192 Karmada]# kubectl get cluster</span><br><span class="line">NAME      VERSION   MODE   READY   AGE</span><br><span class="line">local               Push           3s</span><br><span class="line">member1   v1.19.1   Push   True    27h</span><br><span class="line">member2   v1.19.1   Push   True    27h</span><br><span class="line">member3   v1.19.1   Pull   True    27h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">karmadactl会在纳管的集群中创建service account，并为该service account配置cluster role和cluster role binding</span><br><span class="line">查看到被纳管的集群创建了一个karmada-cluster的ns并创建了对应的sa</span><br><span class="line">[root@192 ~]# kubectl get sa -A | grep karmada</span><br><span class="line">karmada-cluster   default                              1         9m10s</span><br><span class="line">karmada-cluster   karmada-local                        1         9m10s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">查看注册到karmada的集群状态是否正常</span><br><span class="line">[root@192 Karmada]# kubectl get clusters local  [非正常]</span><br><span class="line">NAME    VERSION   MODE   READY   AGE</span><br><span class="line">local             Push           18m</span><br><span class="line">[root@192 Karmada]# kubectl get clusters member1</span><br><span class="line">NAME      VERSION   MODE   READY   AGE</span><br><span class="line">member1   v1.19.1   Push   True    28h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">加入集群异常的问题处理</span><br><span class="line">kubectl get clusters local -o yaml 查看异常的集群</span><br><span class="line">问题排查：</span><br><span class="line">kubectl logs -f karmada-webhook-65fc87f89d-cgh95 -n karmada-system</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">push模式的注销</span><br><span class="line">./karmadactl unjoin member1 --cluster-kubeconfig=$HOME/.kube/karmada.config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">对于push模式的成员集群来说，在karamada控制面中的karamada controller manager会运行一个cluster status controller</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="karmada新建集群"><a href="#karmada新建集群" class="headerlink" title="karmada新建集群"></a>karmada新建集群</h4><p>新建集群会创建两个节点的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">创建一个member-zyx的k8s集群</span><br><span class="line">在控制面执行</span><br><span class="line">hack/create-cluster.sh member-zyx $HOME/.kube/members.config </span><br><span class="line">kubectl config use-context karmada-apiserver</span><br><span class="line">karmadactl join member-zyx --cluster-kubeconfig=$HOME/.kube/members.config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@192 Karmada]# kubectl get  node</span><br><span class="line">NAME                       STATUS   ROLES                  AGE     VERSION</span><br><span class="line">member-zyx-control-plane   Ready    control-plane,master   7m51s   v1.21.1</span><br><span class="line">member-zyx-worker          Ready    &lt;none&gt;                 7m23s   v1.21.1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">部署weave的pod会出错</span><br><span class="line">stackoverflow问题地址：https://stackoverflow.com/questions/63406622/kubernetes-cluster-master-node-shows-notready-coredns-weave-shows-pending/63407681#63407681</span><br><span class="line">自检创建的是weave类型的k8s,查看创建脚本是通过cloud.weave.works网页上下载一个文件的,这个文件不能直接使用的，具体可以参考上面的stack问题地址</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@192 Karmada]# kubectl get  pod</span><br><span class="line">NAME                          READY   STATUS             RESTARTS   AGE</span><br><span class="line">weave-net-rs6ts               0/2     CrashLoopBackOff   12         9m33s</span><br><span class="line"></span><br><span class="line">查看logs</span><br><span class="line">ERRO: 2021/10/14 02:30:43.280200 Failed to destroy ipset &#x27;weave-P.B|!ZhkAr5q=XZ?3&#125;tMBA+0&#x27;</span><br><span class="line">FATA: 2021/10/14 02:30:43.280273 ipset [destroy weave-P.B|!ZhkAr5q=XZ?3&#125;tMBA+0] failed: ipset v7.2: Set cannot be destroyed: it is in use by a kernel component</span><br><span class="line">: exit status 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kind删除集群之后还需要删除新建集群的上下文</span><br><span class="line">kubectl config delete-context member-zyx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">尝试的操作：</span><br><span class="line">对于karmada的解决方案，修改create-cluster的脚本，将下载weave并apply的那部分代码修改下，但是依旧没有效果，</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="部署样例"><a href="#部署样例" class="headerlink" title="部署样例"></a>部署样例</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@192 Karmada]# kubectx</span><br><span class="line">karmada-apiserver</span><br><span class="line">karmada-host</span><br><span class="line"></span><br><span class="line">所有的nginx以及传播策略都要在karmada-apiserver控制面上执行</span><br><span class="line">[root@192 Karmada]# kubectl apply -f samples/nginx/deployment.yaml</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>详见官网，可以让每个集群部署一个</p><!-- ![image-20211009183444986](08%E3%80%81k8s%E9%A1%B9.assets/image-20211009183444986.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/karmada202401161009259.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">控制面板是apiserver</span><br><span class="line"></span><br><span class="line">在控制面上看到的是dp，pod是启在member1集群上的，</span><br><span class="line">[root@192 Karmada]# kubectl get deployment</span><br><span class="line">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx   5/6     6            5           9m9s</span><br><span class="line">[root@192 Karmada]# kubectl get pod</span><br><span class="line">No resources found in default namespace.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">设置member1集群的权重是5，member1集群上也有dp，但是dp中pod的数量是5个</span><br><span class="line">会根据调度策略自动修改dp中pod的数量</span><br><span class="line">[root@192 ~]# kubectl get deploy</span><br><span class="line">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx   5/5     5            5           10m</span><br><span class="line">[root@192 ~]# kubectx member1</span><br><span class="line">Switched to context &quot;member1&quot;.</span><br><span class="line">[root@192 ~]# kubectl get deploy</span><br><span class="line">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx   0/1     1            0           10m</span><br><span class="line"></span><br><span class="line">删除dp之后，dp会自动新建</span><br><span class="line">[root@192 ~]# kubectl  delete deploy nginx</span><br><span class="line">deployment.apps &quot;nginx&quot; deleted</span><br><span class="line">[root@192 ~]# kubectl get deploy</span><br><span class="line">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx   0/1     1            0           1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在karmada的控制面上起了两个work的资源</span><br><span class="line">[root@192 Karmada]# kubectl get work -A</span><br><span class="line">NAMESPACE            NAME               AGE</span><br><span class="line">karmada-es-member1   nginx-687f7fb96f   106m</span><br><span class="line">karmada-es-member2   nginx-687f7fb96f   106m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">模拟其中一个集群故障</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">模拟其中的karmada故障</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="新建集群的weave插件报错"><a href="#新建集群的weave插件报错" class="headerlink" title="新建集群的weave插件报错"></a>新建集群的weave插件报错</h3><p>使用create-cluster脚本时报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建weave插件的cluster报错</span><br><span class="line"></span><br><span class="line">ERRO: 2021/10/09 10:52:25.847207 Failed to destroy ipset &#x27;weave-P.B|!ZhkAr5q=XZ?3&#125;tMBA+0&#x27;</span><br><span class="line">FATA: 2021/10/09 10:52:25.847263 ipset [destroy weave-P.B|!ZhkAr5q=XZ?3&#125;tMBA+0] failed: ipset v7.2: Set cannot be destroyed: it is in use by a kernel component</span><br><span class="line">: exit status 1</span><br><span class="line"></span><br><span class="line">ipset list </span><br></pre></td></tr></table></figure><!-- ![image-20211009192005781](08%E3%80%81k8s%E9%A1%B9.assets/image-20211009192005781.png)  --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/KARMADA_ERROR.png"></p><h3 id="报错：etcd镜像拉去错误"><a href="#报错：etcd镜像拉去错误" class="headerlink" title="报错：etcd镜像拉去错误"></a>报错：etcd镜像拉去错误</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">statefulset.apps/etcd created</span><br><span class="line">service/etcd-client created</span><br><span class="line">service/etcd created</span><br><span class="line">wait the etcd ready...</span><br><span class="line">error: timed out waiting for the condition on pods/etcd-0</span><br><span class="line">kubectl wait --for=condition=Ready --timeout=200s pods -l app=etcd -n karmada-system failed, retrying</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">etcd准备失败，需要拉取国内源的etcd的镜像，修改所有http://k8s.gcr.io镜像。更改 imagePullPolicy: Always 为 imagePullPolicy: IfNotPresent</span><br><span class="line"></span><br><span class="line">export KUBECONFIG=$KUBECONFIG:/root/.kube/karmada.config:$HOME/.kube/karmada-host.config</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# ls .kube/</span><br><span class="line">cache/          ca.pem          config          http-cache/     karmada.config </span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/karmada202401161011520.png"></p><!-- ![image-20210716135407235](..\typora-user-images\image-20210716135407235.png) --><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">image: k8s.gcr.io/etcd:3.4.13-0</span><br><span class="line">在artifacts文件夹中指定了镜像的文件</span><br><span class="line">[root@localhost karmada]# grep &quot;k8s.gcr.io\/&quot;  -r ./*</span><br><span class="line">./artifacts/deploy/karmada-apiserver.yaml:          image: k8s.gcr.io/kube-apiserver:v1.19.1</span><br><span class="line">./artifacts/deploy/karmada-etcd.yaml:          image: k8s.gcr.io/etcd:3.4.13-0</span><br><span class="line"></span><br><span class="line">可以看到etcd的镜像是外网的k8s.gcr.io</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0</span><br></pre></td></tr></table></figure><h3 id="k8s-gcr-io镜像无法下载的问题"><a href="#k8s-gcr-io镜像无法下载的问题" class="headerlink" title="k8s.gcr.io镜像无法下载的问题"></a>k8s.gcr.io镜像无法下载的问题</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">通过如下命令查看对应的版本</span><br><span class="line">kubeadm config images list --kubernetes-version=v1.13.2</span><br><span class="line">k8s.gcr.io/kube-apiserver:v1.13.2</span><br><span class="line">k8s.gcr.io/kube-controller-manager:v1.13.2</span><br><span class="line">k8s.gcr.io/kube-scheduler:v1.13.2</span><br><span class="line">k8s.gcr.io/kube-proxy:v1.13.2</span><br><span class="line">k8s.gcr.io/pause:3.1</span><br><span class="line">k8s.gcr.io/etcd:3.2.24</span><br><span class="line">k8s.gcr.io/coredns:1.2.6</span><br><span class="line"></span><br><span class="line">先从docker.io下载相关镜像，然后再打成k8s的tag</span><br><span class="line">kubeadm version</span><br><span class="line"></span><br><span class="line">kubernetes-version的值可以通过如下命令获取</span><br><span class="line">kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;13&quot;, GitVersion:&quot;v1.13.2&quot;, GitCommit:&quot;cff46ab41ff0bb44d8584413b598ad8360ec1def&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-01-10T23:33:30Z&quot;, GoVersion:&quot;go1.11.4&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">先从docker.io下载相关镜像，然后再打成k8s的tag</span><br><span class="line">vim pull_k8s_images.sh</span><br><span class="line"></span><br><span class="line">#!/bin/sh</span><br><span class="line">### 版本信息</span><br><span class="line">K8S_VERSION=v1.13.2</span><br><span class="line">ETCD_VERSION=3.2.24</span><br><span class="line">DASHBOARD_VERSION=v1.8.3</span><br><span class="line">FLANNEL_VERSION=v0.10.0-amd64</span><br><span class="line">DNS_VERSION=1.14.8</span><br><span class="line">PAUSE_VERSION=3.1</span><br><span class="line">coredns_version=1.2.6</span><br><span class="line">## 基本组件</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:$K8S_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:$K8S_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:$K8S_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/openthings/k8s-gcr-io-kube-proxy-amd64:$K8S_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:$ETCD_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:$PAUSE_VERSION</span><br><span class="line">### 网络</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-sidecar-amd64:$DNS_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-kube-dns-amd64:$DNS_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-dnsmasq-nanny-amd64:$DNS_VERSION</span><br><span class="line">docker pull quay.io/coreos/flannel:$FLANNEL_VERSION</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/openthings/k8s-gcr-io-coredns:$coredns_version</span><br><span class="line">### 前端</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:$DASHBOARD_VERSION</span><br><span class="line"></span><br><span class="line">## 修改tag</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:$K8S_VERSION k8s.gcr.io/kube-apiserver-amd64:$K8S_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:$K8S_VERSION k8s.gcr.io/kube-controller-manager-amd64:$K8S_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:$K8S_VERSION k8s.gcr.io/kube-scheduler-amd64:$K8S_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/openthings/k8s-gcr-io-kube-proxy-amd64:$K8S_VERSION k8s.gcr.io/kube-proxy-amd64:$K8S_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:$ETCD_VERSION k8s.gcr.io/etcd-amd64:$ETCD_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:$PAUSE_VERSION k8s.gcr.io/pause:$PAUSE_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-sidecar-amd64:$DNS_VERSION k8s.gcr.io/k8s-dns-sidecar-amd64:$DNS_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-kube-dns-amd64:$DNS_VERSION k8s.gcr.io/k8s-dns-kube-dns-amd64:$DNS_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-dnsmasq-nanny-amd64:$DNS_VERSION k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:$DNS_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:$DASHBOARD_VERSION k8s.gcr.io/kubernetes-dashboard-amd64:$DASHBOARD_VERSION</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com/openthings/k8s-gcr-io-coredns:$coredns_version  k8s.gcr.io/coredns:$coredns_version</span><br><span class="line">## 删除镜像</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:$K8S_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:$K8S_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:$K8S_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/openthings/k8s-gcr-io-kube-proxy-amd64:$K8S_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:$ETCD_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:$PAUSE_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-sidecar-amd64:$DNS_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-kube-dns-amd64:$DNS_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-dnsmasq-nanny-amd64:$DNS_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:$DASHBOARD_VERSION</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com/openthings/k8s-gcr-io-coredns:$coredns_version</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
          <category> karmada </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> karmada </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubshark</title>
      <link href="/2024/01/15/kubshark/"/>
      <url>/2024/01/15/kubshark/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="江湖传言，Kubeshark身手不凡"><a href="#江湖传言，Kubeshark身手不凡" class="headerlink" title="江湖传言，Kubeshark身手不凡"></a>江湖传言，Kubeshark身手不凡</h1><p>看有人分享了 Kubeshark 这个组件，感觉还是挺有意思，所以这里整理了他收集的一些资料，再结合网上的其他资料，总结了解了一下这个组件。</p><p>Kubeshark 由 2021 年 UP9 公司开源的 K8s API 流量查看器 Mizu 发展而来，试图成为一款 K8s 全过程流量监控工具</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">可以将 Kubeshark 视为 Wireshark、BPF 编译器集合 (BCC) 工具等的 Kubernetes 感知组合。</span><br><span class="line">Kubeshark 可以嗅探集群中的部分或所有 TCP 流量，将其记录到 PCAP 文件中并剖析。</span><br><span class="line">Kubeshark 使用 eBPF 来跟踪内核空间和用户空间中的函数调用。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Kubeshark 由三个不同的软件组成，它们可以协同工作：CLI、Hub 和 Worker。</span><br><span class="line">CLI，它是客户端的 二进制文件，通过 K8s API 与集群通信。</span><br><span class="line">Hub，它协调 worker 部署，接收来自每个 worker 的嗅探和剖析，并收集到一个中心位置。它还提供一个Web界面，用于在浏览器上显示收集到的流量。</span><br><span class="line">Work，作为 DaemonSet 部署到集群中，以确保集群中的每个节点都被 Kubeshark 覆盖。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">https://github.com/kubeshark/kubeshark</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/kube_.png"></p><h2 id="部署安装"><a href="#部署安装" class="headerlink" title="部署安装"></a>部署安装</h2><p>参考官网： <a href="https://docs.kubeshark.co/en/install">https://docs.kubeshark.co/en/install</a></p><h3 id="使用cli的方式安装"><a href="#使用cli的方式安装" class="headerlink" title="使用cli的方式安装"></a>使用cli的方式安装</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">从github上下载的tag包的kubeshark执行的时候有报错</span><br><span class="line">./kubeshark: /lib64/libc.so.6: version `GLIBC_2.34&#x27; not found (required by ./kubeshark)</span><br><span class="line"></span><br><span class="line">看报错原因是因为系统的，该可执行文件需要版本为 GLIBC_2.34 的 libc.so.6 库，但当前系统中没有找到所需的库版本。这可能是因为您的系统的 glibc 版本太旧，不兼容 &quot;kubeshark&quot; 可执行文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">解决方式参考官网</span><br><span class="line">curl -Lo kubeshark https://github.com/kubeshark/kubeshark/releases/download/50.3/kubeshark_linux_amd64 &amp;&amp; chmod 755 kubeshark</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行kubeshak tap的命令后会手动创建下面的pod给你<br><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/202401151903866.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用cli的方式有时候会失败，多运行几次就有可能成功</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="helm的方式安装"><a href="#helm的方式安装" class="headerlink" title="helm的方式安装"></a>helm的方式安装</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">helm repo add kubeshark https://helm.kubeshark.co</span><br><span class="line">helm repo update</span><br><span class="line"></span><br><span class="line">helm install kubeshark kubeshark/kubeshark</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">helm install kubeshark kubeshark/kubeshark -n kubeshark --create-namespace \</span><br><span class="line">--set license=FT7YKA .. 4VGK5EASXETJD2XCWIUVNYAILCJPNNSWX6MSI6V4L5E66XQCJ4SANN3BLGAA= \</span><br><span class="line">--set-json &#x27;scripting.env=&#123;&quot;AWS_ACCESS_KEY_ID&quot;:&quot;AKI..5YF&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;: &quot;mbio..gtJuf&quot;, &quot;AWS_REGION&quot;:&quot;us-east-2&quot;, &quot;S3_BUCKET&quot;:&quot;demo-kubeshark-co&quot;&#125;&#x27; \</span><br><span class="line">--set-json &#x27;tap.annotations=&#123;&quot;eks.amazonaws.com/role-arn&quot;:&quot;arn:aws:iam::74..50:role/s3-role&quot;, &quot;alb.ingress.kubernetes.io/scheme&quot;:&quot;internet-facing&quot;, &quot;alb.ingress.kubernetes.io/target-type&quot;:&quot;ip&quot;&#125;&#x27; \</span><br><span class="line">--set tap.ingress.enabled=true \</span><br><span class="line">--set tap.ingress.host=demo.kubehq.org \</span><br><span class="line">--set &quot;tap.ingress.auth.approveddomains=&#123;kubeshark.co&#125;&quot; \</span><br><span class="line">--set tap.release.namespace=kubeshark \</span><br><span class="line">--set tap.resources.worker.limits.memory=2Gi \</span><br><span class="line">--set-json &#x27;tap.nodeselectorterms=[&#123;&quot;matchExpressions&quot;: [ &#123; &quot;key&quot;: &quot;kubeshark&quot; , &quot;operator&quot;: &quot;In&quot;, &quot;values&quot;: [ &quot;true&quot; ] &#125; ] &#125;]&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="是临时部署还是长期部署"><a href="#是临时部署还是长期部署" class="headerlink" title="是临时部署还是长期部署"></a>是临时部署还是长期部署</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">k8s里面抓包工具，可以临时抓包帮助分析问题。也能看到pod的整个调用链路，抓完就自动清理掉了。相对轻量。[鼓掌]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">为了实现更永久的部署，请添加Kubeshark的 Helm 存储库：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">不管是helm部署还是cli部署都可以实现使用完之后就清理掉</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><h3 id="tap命令"><a href="#tap命令" class="headerlink" title="tap命令"></a>tap命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeshark tap                                       - tap all pods in all namespaces</span><br><span class="line">kubeshark tap --proxy-host 0.0.0.0                  - make the dashboard port accessible from outside localhost</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">监视指定的pod:</span><br><span class="line">kubeshark tap -n sock-shop &quot;(catalo*|front-end*)&quot;   - tap only pods that match the regex in a certain namespace</span><br><span class="line">这个命令的作用是启动 Kubeshark 流量监视器，监视位于 &quot;sock-shop&quot; 命名空间中，并且名称匹配正则表达式 (catalo*|front-end*) 的 Pod 的流量。这样可以有针对性地捕获特定命名空间和名称模式的流量，以进行分析或故障排除</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">监听指定ns的流量:</span><br><span class="line">kubeshark tap -n sock-shop</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">运行在指定ns</span><br><span class="line">kubectl create namespace &lt;unique-name-space&gt;</span><br><span class="line">kubeshark tap --set tap.release.namespace=&lt;unique-name-space&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>默认的tap是监听在localhost的，可以通过port-forward的方式暴露出来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">安装完这些pod之后就可以将front暴露出来，从而实现抓包</span><br><span class="line">kubectl -n default port-forward service/kubeshark-front --address 0.0.0.0 --address :: 30088:80</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><p>特点分析:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work仅解析与支持的协议之一匹配的数据包（例如HTTP、AMQP、Apache Kafka、Redis、gRPC、GraphQL和DNS）。其他协议的数据包不会被解析并被丢弃。</span><br></pre></td></tr></table></figure><h3 id="KFL查询语法"><a href="#KFL查询语法" class="headerlink" title="KFL查询语法"></a>KFL查询语法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">按照目的pod和目的名称空间来筛选流量</span><br><span class="line">dst.name == &quot;echo-other-node&quot; and dst.namespace == &quot;cilium-test&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">安装目的ip来筛选流量</span><br><span class="line">http and src.ip == &quot;192.167.0.22&quot; and src.namespace == &quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">http and request.method == &quot;GET&quot; and request.path != &quot;/example&quot; and (request.query.a &gt; 42 or request.headers[&quot;x&quot;] == &quot;y&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="显示service-map"><a href="#显示service-map" class="headerlink" title="显示service map"></a>显示service map</h3><!-- ![image-20230919141752518](../../../typora%E6%96%87%E4%BB%B6/img/image-20230919141752518.png) --><!-- ![](https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/kub.png) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/kubeshark_map.png"></p><h3 id="kubeshark脚本的作用"><a href="#kubeshark脚本的作用" class="headerlink" title="kubeshark脚本的作用"></a>kubeshark脚本的作用</h3><p>1、检测可疑行为</p><p>2、发送警报</p><p>3、将抓到的包存放在固定的地方</p><h2 id="自动化相关"><a href="#自动化相关" class="headerlink" title="自动化相关"></a>自动化相关</h2><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/k8s/kubeshark202401151906187.png"></p>]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
          <category> kubshark的使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubeshark </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tmux那些事</title>
      <link href="/2024/01/12/tmux%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
      <url>/2024/01/12/tmux%E9%82%A3%E4%BA%9B%E4%BA%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h1><!-- <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=427606780&auto=1&height=66"></iframe>     -->        <div id="aplayer-wDnFlonL" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-wDnFlonL"),            narrow: false,            autoplay: false,            showlrc: false,            music: {              title: "国歌",              author: "群星",              url: "//music.163.com/outchain/player?type=2&id=427606780&auto=1&height=66",              pic: "https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/yasir/%E7%BB%BF%E6%A4%8D.svg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script>        <div id="aplayer-lDYPQnXn" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-lDYPQnXn"),            narrow: false,            autoplay: false,            showlrc: false,            music: {              title: "国歌",              author: "群星",              url: "https://music.163.com/#/song?id=2029245318",              pic: "https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/yasir/%E7%BB%BF%E6%A4%8D.svg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script>        <div id="aplayer-bXkNAEjf" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-bXkNAEjf"),            narrow: false,            autoplay: false,            showlrc: false,            music: {              title: "My Song",              author: "Me",              url: "https://music.163.com/outchain/player?type=2&id=427606780&auto=1&height=66",              pic: "",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script><p>介绍： <mark class="hl-label green">tmux</mark> 是把多个虚拟窗口看成一个物理窗口(terminal multiplexer)，通过tmux的命令来操作这些窗口集合，方便切换且不会打乱你原来的工作环境</p><p>brew install tmux</p><p>tmux 是一个终端复用器，这也是它命名的由来 t(terminal) mux(multiplexer)，你可以在一个屏幕上管理多个终端！</p><p>在 mac 上得益于 iterm2 (opens new window)，你管理多个终端窗口相当方便。但是在 linux 上，iterm2 就鞭长莫及了，tmux 的优势就显出来了。</p><p>tmux的使用说明</p><p><a href="https://www.freeaihub.com/article/tmux.html">https://www.freeaihub.com/article/tmux.html</a></p><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/tmux.png"></p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">术语</span><br><span class="line">server 包含多个 session</span><br><span class="line">session 包含多个 window</span><br><span class="line">window 类似于 iterm2 的 Tab，包含多个 pane，以下中文会翻译为窗口</span><br><span class="line">pane 类似于 iterm2 的 Pane，以下中文会翻译为面板</span><br><span class="line">常见命令</span><br><span class="line">bind-key (bind) ：绑定快捷键，按 prefix 键与快捷键触发。</span><br><span class="line">set-option (set) : 设置选项</span><br><span class="line">source-file (source) : 生效当前配置文件</span><br><span class="line">new-window : 新建窗口，默认快捷键 prefix c</span><br><span class="line">split-window : 分屏</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="使用命令-快捷键"><a href="#使用命令-快捷键" class="headerlink" title="使用命令&amp;快捷键"></a>使用命令&amp;快捷键</h3><p>连接当前的会话<br>tmux attach -t [session-name]- 连接到名为session-name的会话<br>删除会话:tmux kill-session -t &lt;编号&gt;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">2. 创建新的session窗口</span><br><span class="line">tmux new -s new_session</span><br><span class="line">这里的s意为session-name</span><br><span class="line">在new_session中创建新的窗口</span><br><span class="line">第一步：按Ctrl+B 组合键，然后松开。</span><br><span class="line">第二步：再单独按一下c键</span><br><span class="line"></span><br><span class="line">3. 在窗口间切换</span><br><span class="line">假如我们要切换到 0：bash 这个窗口，步骤如下：</span><br><span class="line">第一步：按 Ctrl-B 组合键，然后松开。</span><br><span class="line">第二步：按数字 0 键。</span><br><span class="line"></span><br><span class="line">\4. detach窗口，断开当前会话</span><br><span class="line">第一步：输入组合键 Ctrl+B，然后松开。</span><br><span class="line">第二步：输入字母d。</span><br><span class="line">d即为detach之意</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">\5. 恢复窗口</span><br><span class="line">查看所有窗口：tmux ls</span><br><span class="line">Doc/typora文件  tmux ls</span><br><span class="line">new_session: 4 windows (created Wed Aug 10 10:50:21 2022)</span><br><span class="line">ls意为list-session</span><br><span class="line">恢复指定窗口：tmux a -t new_session</span><br><span class="line">这里-t的意思是target-session</span><br><span class="line"></span><br><span class="line">\6. 切换窗口</span><br><span class="line">在tmux session内切换到另一个tmux session：tmux switch -t xxx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">窗口的拆分</span><br><span class="line">tmux split-window：在当前窗格中水平分割。或者，拆分为上下两个pane：先按下Ctrl+b的组合键，再按下&quot;</span><br><span class="line">tmux split-window -v：在当前窗格中垂直分割。或者拆分为左右两个pane：先按下Ctrl+b的组合键，再按下%</span><br><span class="line">tmux select-pane -L/R/U/D：选择当前窗格的左/右/上/下。</span><br><span class="line">tmux resize-pane -L/R/U/D &lt;size&gt;：调整当前窗格的大小。</span><br><span class="line">tmux list-panes：列出所有窗格。</span><br><span class="line">tmux new-window：在当前会话中创建新窗口。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">删除一个pane(转到选择的pane使用kill-pane命令删除)</span><br><span class="line">tmux kill-pane</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">\2. 在session中切换pane</span><br><span class="line">2.1 切换到左侧的pane：先按下Ctrl+b的组合键，再按下←</span><br><span class="line">2.2 切换到右侧的pane：先按下Ctrl+b的组合键，再按下→</span><br><span class="line">2.3 切换到下方的pane：先按下Ctrl+b的组合键，再按下↓</span><br><span class="line">2.4 切换到上方的pane：先按下Ctrl+b的组合键，再按下↑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">\7. kill掉窗口</span><br><span class="line">kill一个tmux进程：tmux kill-session -t dev（kill名为dev的session）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><!-- ![image-20230414172307252](![](https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/image-20230414172307252.png)) --><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/image-20230414172307252.png" alt="tmux的概念图"></p><p>————-—————这是一个分割线————-————-</p><h3 id="会话-session-命令"><a href="#会话-session-命令" class="headerlink" title="会话(session)命令"></a>会话(session)命令</h3><p>s是常有的命令</p><p>$ 重命名</p><table><thead><tr><th><strong>前缀</strong></th><th><strong>指令</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>Ctrl+b</td><td>?</td><td>显示快捷键帮助文档</td></tr><tr><td>Ctrl+b</td><td>d</td><td>断开当前会话</td></tr><tr><td>Ctrl+b</td><td>D</td><td>选择要断开的会话</td></tr><tr><td>Ctrl+b</td><td>Ctrl+z</td><td>挂起当前会话</td></tr><tr><td>Ctrl+b</td><td>r</td><td>强制重载当前会话</td></tr><tr><td>Ctrl+b</td><td>s</td><td>切换窗口，左右方向键可以展开收缩,常用【重要】</td></tr><tr><td>Ctrl+b</td><td>:</td><td>进入命令行模式，此时可直接输入ls等命令</td></tr><tr><td>Ctrl+b</td><td>[</td><td>进入复制模式，按q退出</td></tr><tr><td>Ctrl+b</td><td>]</td><td>粘贴复制模式中复制的文本</td></tr><tr><td>Ctrl+b</td><td>~</td><td>列出提示信息缓存</td></tr></tbody></table><p><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/20240115170527.png" alt="ctrl+s是一个常用命令"></p><h3 id="窗口（window）指令"><a href="#窗口（window）指令" class="headerlink" title="窗口（window）指令"></a>窗口（window）指令</h3><p>，是重命名</p><table><thead><tr><th><strong>前缀</strong></th><th><strong>指令</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>Ctrl+b</td><td>c</td><td>新建窗口</td></tr><tr><td>Ctrl+b</td><td>&amp;</td><td>关闭当前窗口</td></tr><tr><td>Ctrl+b</td><td>0~9</td><td>切换到指定窗口</td></tr><tr><td>Ctrl+b</td><td>p</td><td>切换到上一窗口</td></tr><tr><td>Ctrl+b</td><td>n</td><td>切换到下一窗口</td></tr><tr><td>Ctrl+b</td><td>w</td><td>打开窗口列表，用于且切换窗口</td></tr><tr><td>Ctrl+b</td><td>,</td><td>重命名当前窗口</td></tr><tr><td>Ctrl+b</td><td>.</td><td>修改当前窗口编号（适用于窗口重新排序）</td></tr><tr><td>Ctrl+b</td><td>f</td><td>快速定位到窗口（输入关键字匹配窗口名称）</td></tr></tbody></table><h3 id="面板（pane）指令"><a href="#面板（pane）指令" class="headerlink" title="面板（pane）指令"></a>面板（pane）指令</h3><table><thead><tr><th><strong>前缀</strong></th><th><strong>指令</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>Ctrl+b</td><td>“</td><td>当前面板上下一分为二，下侧新建面板</td></tr><tr><td>Ctrl+b</td><td>%</td><td>当前面板左右一分为二，右侧新建面板</td></tr><tr><td>Ctrl+b</td><td>x</td><td>关闭当前面板（关闭前需输入y or n确认）</td></tr><tr><td>Ctrl+b</td><td>z</td><td>最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增）</td></tr><tr><td>Ctrl+b</td><td>!</td><td>将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效）</td></tr><tr><td>Ctrl+b</td><td>;</td><td>切换到最后一次使用的面板</td></tr><tr><td>Ctrl+b</td><td>q</td><td>显示面板编号，在编号消失前输入对应的数字可切换到相应的面板</td></tr><tr><td>Ctrl+b</td><td>{</td><td>向前置换当前面板</td></tr><tr><td>Ctrl+b</td><td>}</td><td>向后置换当前面板</td></tr><tr><td>Ctrl+b</td><td>Ctrl+o</td><td>顺时针旋转当前窗口中的所有面板</td></tr><tr><td>Ctrl+b</td><td>方向键</td><td>移动光标切换面板</td></tr><tr><td>Ctrl+b</td><td>o</td><td>选择下一面板</td></tr><tr><td>Ctrl+b</td><td>空格键</td><td>在自带的面板布局中循环切换</td></tr><tr><td>Ctrl+b</td><td>Alt+方向键</td><td>以5个单元格为单位调整当前面板边缘</td></tr><tr><td>Ctrl+b</td><td>Ctrl+方向键</td><td>以1个单元格为单位调整当前面板边缘</td></tr><tr><td>Ctrl+b</td><td>t</td><td>显示时钟</td></tr></tbody></table><h4 id="显示序号"><a href="#显示序号" class="headerlink" title="显示序号"></a>显示序号</h4><p><img src="/../../typora%E6%96%87%E4%BB%B6/img/image-20230414175247276.png" alt="image-20230414175247276"></p><h4 id="显示时钟"><a href="#显示时钟" class="headerlink" title="显示时钟"></a>显示时钟</h4><p>ctrl+b t<br><img src="https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/img/20240115165406.png" alt="显示时钟"></p><h2 id="命令行模式"><a href="#命令行模式" class="headerlink" title="命令行模式"></a>命令行模式</h2><p>ctrl b : 进入命令行模式</p><p>要退出tmux的命令行模式，请按下 <code>Ctrl + b</code>，然后再按下 <code>Esc</code> 键。这将使您退出命令行模式并返回到普通的tmux窗口模式。</p><p>您还可以按下 <code>Ctrl + c</code> 或 <code>q</code> 来取消当前命令并返回到窗口模式。</p><blockquote><p>使用ess或者 ctrl+c或者q凯返回到窗口模式</p></blockquote><p>请注意，当您在tmux中使用命令时，会进入命令行模式。在这种模式下，您可以输入并执行tmux命令。要退出命令行模式并返回到窗口模式，您需要使用 <code>Esc</code>、<code>Ctrl + c</code> 或 <code>q</code> 等命令取消当前命令。如果您忘记了要执行的命令或不想执行任何命令，可以按下 <code>Esc</code> 或 <code>Ctrl + c</code> 来取消当前命令并返回到窗口模式</p><h2 id="酷炫功能"><a href="#酷炫功能" class="headerlink" title="酷炫功能"></a>酷炫功能</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">面板同步[已测试]：</span><br><span class="line">使用 Ctrl + b + : 进入命令模式，然后输入 setw synchronize-panes on 命令可以使所有面板同时输入相同的内容，</span><br><span class="line">非常适合同时在多个服务器上执行相同的操作。</span><br><span class="line">setw synchronize-panes off可以关闭这个功能</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">插件支持：tmux有一个强大的插件生态系统，您可以使用插件来增强其功能。</span><br><span class="line">例如，tmux-resurrect插件可以将您的tmux会话保存在磁盘上，以便在下次启动时自动还原会话。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">以下是一些tmux的酷炫功能：</span><br><span class="line">窗口和窗格布局：tmux允许您在同一个终端窗口中同时运行多个会话，您可以通过分割窗格或在不同的窗口之间切换来组织这些会话。</span><br><span class="line"></span><br><span class="line">会话管理：tmux支持多个会话，您可以使用 tmux new-session 命令创建新会话，使用 tmux switch -t &lt;session-name&gt; 命令切换到不同的会话，并使用 tmux list-sessions 命令查看所有会话。</span><br><span class="line"></span><br><span class="line">复制和粘贴：tmux允许您在不同的窗格之间复制和粘贴文本。您可以使用 Ctrl + b + [ 进入复制模式，然后使用箭头键选择要复制的文本，最后使用 Enter 键将其复制到剪贴板。然后，在另一个窗格中使用 Ctrl + b + ] 粘贴该文本。</span><br><span class="line"></span><br><span class="line">面板缩放：使用 Ctrl + b + Z 可以将当前活动窗格最大化或最小化</span><br><span class="line"></span><br><span class="line">状态栏自定义：</span><br><span class="line">使用 set-option -g status-right 命令可以自定义tmux状态栏，</span><br><span class="line">例如，您可以在状态栏中显示日期和时间、主机名、当前目录等等。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">快捷键自定义：</span><br><span class="line">tmux允许您自定义快捷键，您可以根据自己的需要创建自己的快捷键。例如，您可以使用 unbind-key 命令解除默认的 Ctrl + b 绑定，然后将其替换为您自己喜欢的键。</span><br><span class="line">总之，tmux是一个非常强大和灵活的终端多路复用器，具有许多高级功能和自定义选项，可以让您更有效地</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="自定义配置"><a href="#自定义配置" class="headerlink" title="自定义配置"></a>自定义配置</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">自定义状态栏的背景和前景色</span><br><span class="line">默认的背景色为 绿色，前景色为 黑色，笔者分别调整为 黑色 和 黄色</span><br><span class="line">set -g status-bg black</span><br><span class="line">set -g status-fg yellow</span><br><span class="line"></span><br><span class="line">自定义状态栏的信息显示</span><br><span class="line">默认在状态栏右侧显示 主机名 + 时间 + 日期</span><br><span class="line">因为笔者一般使用 oh-my-zsh，已经配置显示了 主机名 + 时间 + 日期，所以改为只保留 时间（对比见上图）</span><br><span class="line">set-option -g status-right &quot;#(date +%H:%M&#x27; &#x27;)&quot;</span><br><span class="line">set-option -g status-right-length 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">自定义窗口下标起始</span><br><span class="line">Tmux 可以新建很多窗口，默认窗口标号为 0，然后才是 1 2 3 ...（窗格也是）</span><br><span class="line">虽然很符合程序思维，但是跟键盘上那一排数字键的顺序不吻合，所以改成 1 开始，然后才是 2 3 4 ...</span><br><span class="line">set-option -g base-index 1</span><br><span class="line">set-window-option -g pane-base-index 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">自定义快捷键：分割窗格</span><br><span class="line">默认情况下：</span><br><span class="line">水平分割：ctrl-b + &quot;（引号键）</span><br><span class="line">垂直分割：ctrl-b + %（百分号键）</span><br><span class="line">改成 - 和 |，更符合行为逻辑</span><br><span class="line"></span><br><span class="line">unbind &#x27;&quot;&#x27;</span><br><span class="line">bind - splitw -v</span><br><span class="line">unbind %</span><br><span class="line">bind | splitw -h </span><br><span class="line"></span><br><span class="line">bind-key - split-window -c &quot;#&#123;pane_current_path&#125;&quot;</span><br><span class="line">bind-key | split-window -h -c &quot;#&#123;pane_current_path&#125;&quot;</span><br><span class="line">最后两行表示，通过 - 和 | 分割出的新窗格，shell 的初始路径为当前路径</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="调整pane的大小"><a href="#调整pane的大小" class="headerlink" title="调整pane的大小"></a>调整pane的大小</h3><table><thead><tr><th>Ctrl+方向键</th><th>以1个单元格为单位移动边缘以调整当前窗格大小</th></tr></thead><tbody><tr><td>Alt+方向键</td><td>以5个单元格为单位移动边缘以调整当前窗格大小</td></tr></tbody></table><p>在tmux中,可以通过以下几种方式调整pane的大小:</p><ol><li>使用鼠标拖动pane边界<br>当tmux启动鼠标支持(使用-m或ctrl+b [)后,可以直接用鼠标拖动pane边界来调整pane大小,这是最简单的调整方式。</li><li>使用resize-pane命令<br>使用resize-pane命令可以精确调整pane大小,命令格式为:<br>例如,调整当前pane向右5个单元格:<br>resize-pane -R 5</li></ol><h2 id="配置恢复"><a href="#配置恢复" class="headerlink" title="配置恢复"></a>配置恢复</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">配置安装参考：https://zhuanlan.zhihu.com/p/533349040</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">由于使用了 tmux-continuum，tmux-resurrect 的备份和恢复都是自动的（在指定时间间隔备份、在重启后进行恢复）。当然，你也可以直接调用 tmux-resurrect 来进行手动的备份与恢复，从而实现更灵活的操作：</span><br><span class="line">手动备份tmux：Ctrl+b然后按Ctrl+s</span><br><span class="line">手动恢复tmux：Ctrl+b然后按Ctrl+r</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
          <category> tmux的使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tmux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/01/01/hello-world/"/>
      <url>/2022/01/01/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>        <div id="aplayer-iLpmMfCM" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-iLpmMfCM"),            narrow: false,            autoplay: false,            showlrc: false,            music: {              title: "国歌",              author: "群星",              url: "https://music.163.com/#/song?id=2029245318",              pic: "https://cdn.jsdelivr.net/gh/zhouyaxiong/yasir_picture/yasir/%E7%BB%BF%E6%A4%8D.svg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script><!-- <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src=""></iframe>   --><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
          <category> hexo的使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Frontend </tag>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
